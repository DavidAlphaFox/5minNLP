# 5分钟NLP-信息提取

关键词提取有两种路径：

1、有监督，基于词表对文章打标签

2、无监督方法：tf_idf , text_rank, 主题模型

有监督就是词表遍历。下面就来看看无监督方法

### 1、TF_IDF 关键词提取

TF_IDF = Term Frequency  x   Inverse Document Frequency  = 词频 x 逆文档频次

**TF 词频** =  word 在文档中的次数 / 文档总词数   = 统计词在文档中出现的次数 
**IDF 逆文档频次** = log(文档总数/  (1+包含词的文档数))  =  统计一个词在文档及的多少个文档中出现。如果一个词在越少的文档中出现，则其对文档的区分能力也就越强。 

```c#
例如: 献血在1000篇文档中的10篇出现,献血总次数，tf= 0.02, 那么TF-IDF = 0.02 x 1000 /(1+10) 
```

```
python 库
from sklearn import feature_extraction
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
TfidfVectorizer
```



另外可以考虑 词性，文章中的开头结尾位置,进行加权。

### 2、text_rank 关键词提取

基于单篇文档，提取文档关键字

```
PageRank 思想:  对网页根据链接建立一个有向图，权重=链接数量 X 链接质量 
Weight(Vi) = 1+d-d x Sum( Weight(Vj)/vjOut出链数  ）   d 为解决孤立节点
```

那text_rank 就要基于单篇文档构建这个有向图，思路为：设置一个长度为N的滑动窗口，所有在这个窗口之内的词都视作词结点的相邻结点。

![image-20201015092738039](5%E5%88%86%E9%92%9F%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96.assets/image-20201015092738039.png)

具体实现：

- 建立关键词图G = (V,E)，其中V为节点集，

- 然后采用窗口中词的共现关系（co-occurrence）构造任两点之间的边，两个节点之间存在边仅当它们对应的词汇在长度为K的窗口中共现，K表示窗口大小，即最多共现K个单词，

- 迭代传播各节点的权重，直至收敛

  ```
  python 库   TextRank4Keyword
  ```

  

### 3、自动摘要-关键句提取

自动摘要思想和text_rank 类似，只是建立句子间的关联

1、把句子间的相似度作为句子的关联度，去掉相似度较低的边连接，构建出节点连接图
2、计算SentenceRank值，选出SentenceRank值最高的几个节点对应的句子作为摘要。

```
python 库  TextRank4Sentence
          from  jieba import analyse.textrank
```



### 4、语义信息提取- 主题模型引入



文章通篇介绍了狮子，老虎，鳄鱼等，但是没有出现动物这个词，这时，相关性的时候需要考虑到语义，而语义挖掘的利器是主题模型，把每个文档对应一个到多个主题，每个主题有对应的词分布。



主题模型：  
``` 
p(wi|dj) = sum(P(wi|tk) x P(tk|dj))
词|文档          词|主题     主题|文档
```
目前P(wi|dj)  已知，即一个词在文档中的概率已知，要得到  P(wi|tk) x P(tk|dj)  

主题模型的求解方法：

​        LSA/LSI   潜在语义分析/潜在语义索引： svd 奇异值分解
​        LDA 隐含狄利克雷分布 ： 贝叶斯方法

### 5 、LSA/LSI  语义信息提取 - 潜在语义索引  

LSA/LSI  潜在语义索引 = Latent Semantic   Analyse/ Indexing

##### 5.1 基本思路：

 通过SVD 分解得到隐语义矩阵。

##### 5.2 流程： 

**1- 分析文档集合，建立词汇-文本矩阵A**。
         词汇-文本矩阵A是一个稀疏矩阵，其行代表词语，其列代表文档

![这里写图片描述](5%E5%88%86%E9%92%9F%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96.assets/20180605102300358)





**2- 对词汇-文本矩阵进行奇异值SVD分解。X x B x Y**

```
X 是对词进行分类的一个结果        
    它的每一行表示一个词，每一列表示一个语义相近的词类，这一行中每个非零元素表示每个词在每个语义类中的重要性，数越大，表明和词类越重要，例如词出现两遍，这个值就会大一点
B 表示词的类和文章的类之间的相关性 
Y 是对文本进行分类的一个结果       
    它的每一行表示一个主题，每一列表示一个文本，这一列每个元素表示这篇文本在不同主题中的相关性
```

分解后的示例图如下：     x 为词和词类的关系，B3x3  表示词类和隐主题的关系， Y 为文档和隐主题的关系 

![这里写图片描述](5%E5%88%86%E9%92%9F%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96.assets/20180605102329453)



##### **5.3- 应用**

有了这个语义关系之后，我们可以对词和文档进行相似度计算。得到top_k, 语义近义词 等等

```
python 库
from  gensim import corpora, models from gensim.similarities import Similarity
```



### 6、LDA 模型-主题提取

LDA = Latent Dirichlet Allocation     隐狄利克雷分配模型

##### 6.1 基本原理

LDA采用生成模型，认为**一篇文章的每个词都是通过“以一定概率选择了某个主题，并从这个主题中以一定概率选择某个词语”**这样一个过程得到的。那么，如果我们要生成一篇文档，它里面的每个词语出现的概率为：

P(词语|文档) = Sum主题( P(词语|主题) x P(主题|文档))

矩阵模型：

![image-20201015115000430](5%E5%88%86%E9%92%9F%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96.assets/image-20201015115000430.png)

给定一系列文档，通过对文档进行分词，计算各个文档中每个单词的词频就可以得到左边这边”文档-词语”矩阵。主题模型就是通过左边这个矩阵进行训练，学习出右边两个矩阵。



##### 6.2 LDA模型

###### 6.2.1 原理   从先验分布, beta分布到Dirichlet 分布

LDA是基于贝叶斯模型的，即，**先验分布** + 数据（似然）= 后验分布

```
贝叶斯模型  你对好人和坏人的认知，先验分布为：100个好人和100个的坏人，即你认为好人坏人各占一半，现在你被2个好人（数据）帮助了和1个坏人骗了，于是你得到了新的后验分布为：102个好人和101个的坏人。现在你的后验分布里面认为好人比坏人多了。这个后验分布接着又变成你的新的先验分布，当你被1个好人（数据）帮助了和3个坏人（数据）骗了后，你又更新了你的后验分布为：103个好人和104个的坏人。依次继续更新下去。
```

 数据似然用一个二项分布表示为：

![image-20201017115532200](5%E5%88%86%E9%92%9F%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96.assets/image-20201017115532200.png)

其中p我们可以理解为好人的概率，k为好人的个数，n为好人坏人的总数。

```
先验分布和后验分布的形式应该是一样的，这样的分布我们一般叫共轭分布。虽然数据(似然)很好理解，因为我们希望这个先验分布和数据（似然）对应的二项分布集合后，得到的后验分布在后面还可以作为先验分布！
上面例子里的“102个好人和101个的坏人”，它是前面一次贝叶斯推荐的后验分布，又是后一次贝叶斯推荐的先验分布。也即是说，我们希望先验分布和后验分布的形式应该是一样的，这样的分布我们一般叫共轭分布。
```

二项分布共轭的分布其实就是**Beta分布**

​	![image-20201017132216930](5%E5%88%86%E9%92%9F%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96.assets/image-20201017132216930.png)

其中ΓΓ是Gamma函数，满足Γ(x)=(x−1)!

![image-20201017133525268](5%E5%88%86%E9%92%9F%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96.assets/image-20201017133525268.png)



将上面最后的式子归一化以后，得到我们的后验概率为：

![image-20201017133422574](5%E5%88%86%E9%92%9F%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96.assets/image-20201017133422574.png)

这个式子完全符合我们在上一节好人坏人例子里的情况，我们的认知会把数据里的好人坏人数分别加到我们的先验分布上，得到后验分布。　

**Dirichlet分布**

三项的多项分布好表达，我们假设数据中的第一类有m1 个好人，第二类有m2 个坏人，第三类为m3=n−m1−m2 个不好不坏的人,对应的概率分别为p1,p2,p3=1−p1−p2p1,p2,p3=1−p1−p2，则对应的多项分布为：

![image-20201017115459834](5%E5%88%86%E9%92%9F%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96.assets/image-20201017115459834.png)

那三维的Beta分布呢？超过二维的Beta分布我们一般称之为狄利克雷(以下称为Dirichlet )分布。也可以说Beta分布是Dirichlet 分布在二维时的特殊形式。从二维的Beta分布表达式，我们很容易写出三维的Dirichlet分布如下：

![image-20201017133930800](5%E5%88%86%E9%92%9F%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96.assets/image-20201017133930800.png)

​                               后验概率为下一个的先验概率。

###### 6.2.2 LDA 模型

文档集合表示为

![image-20201017143420222](5%E5%88%86%E9%92%9F%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96.assets/image-20201017143420222.png)



我们需要先假定一个主题数目K，这样所有的分布就都基于K个主题展开

![img](5%E5%88%86%E9%92%9F%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96.assets/1042406-20170517134339588-825441177.png)`

```
文档主题的先验分布是Dirichlet分布，即对于任一文档d, 其主题分布θd为： θd=Dirichlet(α ) ， 其中，α为分布的参数，是一个K维向量
假设主题中词的先验分布是Dirichlet分布，即对于任一主题k, 其词分布βk为：βk=Dirichlet(η)
对于任一一篇文档d中的第n个词，从主题分布θd中得到它的主题编号Zdn的分布为：Zdn=multi(θd)   
而对于该主题编号，得到我们看到的词Wdn的概率分布为： Wdn=multi(βzdn)
这个模型里，我们有MM个文档主题的Dirichlet分布，而对应的数据有MM个主题编号的多项分布，这样(α→θd→Zd)就组成了Dirichlet-multi共轭
```



θd的后验分布：![image-20201017135848133](5%E5%88%86%E9%92%9F%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96.assets/image-20201017135848133.png)

​       如果在第d个文档中，第k个主题的词的个数为：![image-20201017143500717](5%E5%88%86%E9%92%9F%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96.assets/image-20201017143500717.png), 多项分布计数表示为![image-20201017143515517](image-20201017143515517.png)

βk的后验分布为：![image-20201017135925034](5%E5%88%86%E9%92%9F%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96.assets/image-20201017135925034.png)

​        在第k个主题中，第v个词的个数为：![image-20201017140020645](5%E5%88%86%E9%92%9F%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96.assets/image-20201017140020645.png)，向量 ![image-20201017140040436](5%E5%88%86%E9%92%9F%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96.assets/image-20201017140040436.png)

##### 6.3  求解lda 模型- 吉布斯采样

###### 吉布斯采样

吉布斯采样算法的基本原理通过随机采样不断更新模体模型及其在各条输入序列中出现的位置，优化目标函数，当满足一定的迭代终止条件或者达到最大迭代次数时就得到了最终所求的模型。

假设已得到样本Xi,记下一个样本为

![img](https://bkimg.cdn.bcebos.com/formula/82cc61ad87f8eeb494730ad3a87edaa2.svg)

对其中某一分量![img](5%E5%88%86%E9%92%9F%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96.assets/fd1bdfc0a266d2fa48fd4289035651cd.svg),我们根据条件概率![img](5%E5%88%86%E9%92%9F%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96.assets/d61be409606ea4bdd994c3ea74dd2835.svg)

来抽取，即根据已知样本（相当于更新模型参数）一个个抽取。

```
如果我们通过采样得到了所有词的主题,那么通过统计所有词的主题计数，就可以得到各个主题的词分布。接着统计各个文档对应词的主题计数，就可以得到各个文档的主题分布。
```

![image-20201017144710688](5%E5%88%86%E9%92%9F%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96.assets/image-20201017144710688.png)

###### **训练流程**

　　　　1） 选择合适的主题数KK, 选择合适的超参数向量α ,η

　　　　2） 对应语料库中每一篇文档的每一个词，随机的赋予一个主题编号z

　　　　3) 重新扫描语料库，对于每一个词，利用Gibbs采样公式更新它的topic编号，并更新语料中该词的编号。

```
如果令该单词对应的主题为tj,那么Pj(wi|ds)=P(wi|tj)∗P(tj|ds)；接下来枚举T中的topic，得到所有的pj(wi|ds)，其中j取值1~k。然后可以根据这些概率值结果为ds中的第i个单词wi选择一个topic。
例如：最简单的想法是取令pj(wi|ds)，即argmax[j]pj(wi|ds) 
```

<img src="5%E5%88%86%E9%92%9F%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96.assets/image-20201017155647660.png" alt="image-20201017155647660" style="zoom: 80%;" />

　     w1这个词的主题  = max[文档doc1 中包含的主题topic_i词个数 *  和这个词在主题topic_i中的个数]的主题

　　　4） 重复第3步的基于主题轴轮换的Gibbs采样，直到Gibbs采样收敛。对文档集D中的所有文档d中的所有w进行一次p(w|d)计算，并重新选择主题看成是一次迭代

```
            收敛=每篇文章和主题和每个词和主题基本固定下来了，或者说不变了
```



　　　　5） 统计语料库中的各个文档各个词的主题，得到文档主题分布θd，统计语料库中各个主题词的分布，得到LDA的主题与词的分布βk。



###### **预测流程**

　　　　1） 对应当前文档的每一个词，随机的赋予一个主题编号z

　　　　2)   重新扫描当前文档，对于每一个词，利用Gibbs采样公式更新它的topic编号。

　　　　3） 重复第2步的基于主题轴轮换的Gibbs采样，直到Gibbs采样收敛。

　　　　4） 统计文档中各个词的主题，得到该文档主题分布。

##### 6.4  求解lda 模型- 变分推断EM算法 

应用于Spark MLlib和Scikit-learn的LDA算法实现  

###### 从极大似然估计到EM算法

**极大似然估计**：已知模型，评估参数的方法。

```
假如在前面的一百次重复记录中，有七十次是白球，请问罐中白球所占的比例最有可能是多少？
P(样本结果|Model) = p^70  (1-p)^30  
p 可能是50%, %30 ..... ,    
极大似然估计采取的方法是让这个样本结果出现的可能性最大，也就是使得p^70(1-p)^30值最大，那么我们就可以看成是p的方程，求导即可！
```

**EM算法:**  带有隐变量的概率模型的估计

```
例如：我手上的数据是四川人和东北人的身高合集，然而对于其中具体的每一个数据，并没有标定出它来自“东北人”还是“四川人”，需要估计四川人的身高均值。
我们考虑隐变量 z  表示 来自四川还是东北。
```

EM算法是这样做的：

　　首先，初始化参数θ，例如先初始化先 四川人身高的正态分布的参数：如均值=1.65，方差=0.15

　　（1）E-Step：根据参数θ计算每个样本属于zi的概率，即这个身高来自四川或东北的概率，这个概率就是Q

　　（2）M-Step：根据计算得到的Q，求出含有θ的似然函数的下界并最大化它，得到新的参数θ

　　重复（1）和（2）直到直到参数不发生变化为止，收敛，可以看到，从思想上来说，和最开始没什么两样，只不过直接最大化似然函数不好做，曲线救国而已。

**EM算法on LDA** 

EM算法在这里的使用，我们的模型里面有隐藏变量θ,β,zθ,β,z，模型的参数是α,ηα,η。为了求出模型参数和对应的隐藏变量分布，EM算法需要在E步先求出隐藏变量θ,β,zθ,β,z的基于条件概率分布的期望，接着在M步极大化这个期望，得到更新的后验模型参数α,ηα,η。

变分推断，也就是在隐藏变量存在耦合的情况下，我们通过变分假设，即假设所有的隐藏变量都是通过各自的独立分布形成的，这样就去掉了隐藏变量之间的耦合关系。

###### 算法流程

```
输入：主题数KK,M个文档与对应的词。
　　　　1） 初始化α,η向量。
　　　　2）开始EM算法迭代循环直到收敛。
　　　　　　a) 初始化所有的ϕ,γ,λ，进行LDA的E步迭代循环,直到λ,ϕ,γ收敛。
　　　　　　　　(i) for d from 1 to M:
　　　　　　　　　  　for n from 1 to Nd:
　　　　　　　　　　　  　for k from 1 to K:
　　　　　　　　　　　　　　　　更新ϕdnk    
　　　　　　　　　　　  标准化ϕnk使该向量各项的和为1.
　　　　　　　　　　按照(24) 式更新γdk。
　　　　　　　　(ii) for k from 1 to K:
　　　　　　　　　　　　for i from 1 to V:
　　　　　　　　　　          更新λkiλki。
　　　　　　　　(iii)如果ϕ,γ,λ均已收敛，则跳出a)步，否则回到(i)步。
　　　　　　b) 进行LDA的M步迭代循环， 直到α,ηα,η收敛
　　　　　　　    牛顿法迭代更新α,ηα,η直到收敛
　　　　　　c) 如果所有的参数均收敛，则算法结束，否则回到第2)步。
```



##### 6.5 python LDA库

算法很复杂，python 已有库，使用很简单。

```php
from gensim import corpora, models, similarities
 
lda = models.LdaModel(corpus_tfidf, num_topics=num_topics, id2word=dictionary,
                      alpha=0.01, eta=0.01, minimum_probability=0.001,
                      update_every=1, chunksize=100, passes=1)
for i in idx:
        topic = np.array(doc_topics[i])
        topic_distribute = np.array(topic[:, 1])
        # print topic_distribute
        topic_idx = topic_distribute.argsort()[:-num_show_topic - 1:-1]
        print('第%d个文档的前%d个主题：' % (i, num_show_topic)), topic_idx
        print(topic_distribute[topic_idx]) 
            
print('8.结果：每个主题的词分布：--')
for topic_id in range(num_topics):
        print('主题#%d：\t' % topic_id)
        term_distribute_all = lda.get_topic_terms(topicid=topic_id)
        term_distribute = term_distribute_all[:num_show_term]
        term_distribute = np.array(term_distribute)
        term_id = term_distribute[:, 0].astype(np.int)
        print('词：\t', )
        for t in term_id:
            print(dictionary.id2token[t], )
```



参考：   

https://www.cnblogs.com/pinard/p/6831308.html  LDA 基础

https://www.cnblogs.com/pinard/p/6873703.html  gibbs   

https://www.cnblogs.com/pinard/p/6873703.html  em lda 

https://www.cnblogs.com/USTC-ZCC/p/10845506.html em

 视频： youtube 上的LDA

https://www.bilibili.com/video/BV1t54y127U8?from=search&seid=445705003298445410

